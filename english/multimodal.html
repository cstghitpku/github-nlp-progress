<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Multimodal</title>
    <meta name="description" content="">
    <meta name="generator" content="VuePress 1.3.0">
    
    
    <link rel="preload" href="/github-nlp-progress/assets/css/0.styles.6033efd5.css" as="style"><link rel="preload" href="/github-nlp-progress/assets/js/app.e13193a8.js" as="script"><link rel="preload" href="/github-nlp-progress/assets/js/2.3456af6e.js" as="script"><link rel="preload" href="/github-nlp-progress/assets/js/3.937c9f03.js" as="script"><link rel="preload" href="/github-nlp-progress/assets/js/27.cd03ef98.js" as="script"><link rel="prefetch" href="/github-nlp-progress/assets/js/10.cb15689e.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/11.a5465276.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/12.2b17b66d.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/13.d412462c.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/14.fef37947.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/15.ecc915a5.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/16.4815e1b9.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/17.d1b3fb2c.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/18.2660aa61.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/19.bc447758.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/20.ce794ac0.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/21.eb304340.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/22.e6fa021a.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/23.1ae8cb4e.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/24.a3f3a9e6.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/25.cb1417bc.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/26.00aae365.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/28.b10f1382.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/29.56a989c1.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/30.e8c1c8e5.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/31.01e163c8.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/32.e31e82a1.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/33.65044c7c.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/34.623b67c0.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/35.42def8dc.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/36.1a343f82.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/37.f25097ce.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/38.6509d850.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/39.a34af712.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/4.8331929d.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/40.f6e3ec89.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/41.00e83a9b.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/42.43322260.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/43.3f5f54df.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/44.cc4458ba.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/45.c2da54c0.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/46.487e4ae3.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/47.af0bd253.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/48.1bd71c87.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/49.ac4a3e6f.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/5.ffcec782.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/50.2f37b35e.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/51.85bac491.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/52.c7178c1f.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/53.656193ed.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/54.492bbf22.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/55.ef978c39.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/56.86514030.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/57.1a8194c3.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/58.acce0f04.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/59.429ce87d.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/6.ad1599e0.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/60.080bacc4.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/61.4e3f36ed.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/62.d8654f7a.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/63.9d3d2999.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/7.66c4c13e.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/8.58e786b6.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/9.65da761c.js">
    <link rel="stylesheet" href="/github-nlp-progress/assets/css/0.styles.6033efd5.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div id="global-layout" data-v-2be041bb><header class="bk-dark" data-v-2be041bb><video autoplay="autoplay" loop="loop" muted="muted" data-v-2be041bb><source src="/github-nlp-progress/assets/media/bk.417d52db.mp4" type="video/mp4" data-v-2be041bb></video> <div data-v-2be041bb><div class="header-content" data-v-2be041bb><h1 data-v-2be041bb>NLP-PROGRESS</h1> <h2 data-v-2be041bb>Repository to trasck the progress in Natural Language Processing (NLP), including the datasets and the current state-of-the-art for the most common NLP tasks.</h2> <a href="#" class="btn" data-v-2be041bb><i class="iconfont icon-github" data-v-2be041bb></i>
                    View on GitHub
                </a></div></div></header> <div class="theme-container no-navbar" data-v-2be041bb><!----> <div class="sidebar-mask"></div> <div class="sidebar-wrap sidebar-wrap-absolute"><aside class="sidebar"><!---->  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>目录</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/github-nlp-progress/english/multimodal.html#multimodal-emotion-recognition" class="sidebar-link">Multimodal Emotion Recognition</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/github-nlp-progress/english/multimodal.html#iemocap" class="sidebar-link">IEMOCAP</a></li></ul></li><li><a href="/github-nlp-progress/english/multimodal.html#multimodal-metaphor-recognition" class="sidebar-link">Multimodal Metaphor Recognition</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/github-nlp-progress/english/multimodal.html#multimodal-sentiment-analysis" class="sidebar-link">Multimodal Sentiment Analysis</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/github-nlp-progress/english/multimodal.html#mosi" class="sidebar-link">MOSI</a></li></ul></li></ul></section></li></ul> </aside></div> <main class="page"> <div class="theme-default-content content__default"><h1 id="multimodal"><a href="#multimodal" class="header-anchor">#</a> Multimodal</h1> <h2 id="multimodal-emotion-recognition"><a href="#multimodal-emotion-recognition" class="header-anchor">#</a> Multimodal Emotion Recognition</h2> <h3 id="iemocap"><a href="#iemocap" class="header-anchor">#</a> IEMOCAP</h3> <p>The  IEMOCAP (<a href="https://link.springer.com/article/10.1007/s10579-008-9076-6" target="_blank" rel="noopener noreferrer">Busso  et  al., 2008<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>) contains the acts of 10 speakers in a two-way conversation segmented into utterances. The medium of the conversations in all the videos is English. The database contains the following categorical labels: anger, happiness, sadness, neutral, excitement, frustration, fear, surprise,  and other.</p> <p><strong>Monologue:</strong></p> <table><thead><tr><th>Model</th> <th style="text-align:center;">Accuracy</th> <th>Paper / Source</th></tr></thead> <tbody><tr><td>CHFusion (Poria et al., 2017)</td> <td style="text-align:center;">76.5%</td> <td><a href="https://arxiv.org/pdf/1806.06228.pdf" target="_blank" rel="noopener noreferrer">Multimodal Sentiment Analysis using Hierarchical Fusion with Context Modeling<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>bc-LSTM (Poria et al., 2017)</td> <td style="text-align:center;">74.10%</td> <td><a href="http://sentic.net/context-dependent-sentiment-analysis-in-user-generated-videos.pdf" target="_blank" rel="noopener noreferrer">Context-Dependent Sentiment Analysis in User-Generated Videos<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr></tbody></table> <p><strong>Conversational:</strong>
Conversational setting enables the models to capture emotions expressed by the speakers in a conversation. Inter speaker dependencies are considered in this setting.</p> <table><thead><tr><th>Model</th> <th style="text-align:center;">Weighted Accuracy (WAA)</th> <th>Paper / Source</th></tr></thead> <tbody><tr><td>CMN (Hazarika et al., 2018)</td> <td style="text-align:center;">77.62%</td> <td><a href="http://aclweb.org/anthology/N18-1193" target="_blank" rel="noopener noreferrer">Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>Memn2n</td> <td style="text-align:center;">75.08</td> <td><a href="http://aclweb.org/anthology/N18-1193" target="_blank" rel="noopener noreferrer">Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr></tbody></table> <h2 id="multimodal-metaphor-recognition"><a href="#multimodal-metaphor-recognition" class="header-anchor">#</a> Multimodal Metaphor Recognition</h2> <p><a href="http://www.aclweb.org/anthology/S16-2003" target="_blank" rel="noopener noreferrer">Mohammad et. al, 2016<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> created a dataset of verb-noun pairs from WordNet that had multiple senses. They annoted these pairs for metaphoricity (metaphor or not a metaphor). Dataset is in English.</p> <table><thead><tr><th>Model</th> <th style="text-align:center;">F1 Score</th> <th>Paper / Source</th> <th>Code</th></tr></thead> <tbody><tr><td>5-layer convolutional network (Krizhevsky et al., 2012), Word2Vec</td> <td style="text-align:center;">0.75</td> <td><a href="http://www.aclweb.org/anthology/N16-1020" target="_blank" rel="noopener noreferrer">Shutova et. al, 2016<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td>Unavailable</td></tr></tbody></table> <p><a href="http://www.aclweb.org/anthology/P14-1024" target="_blank" rel="noopener noreferrer">Tsvetkov  et. al, 2014<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> created a dataset of adjective-noun pairs that they then annotated for metaphoricity. Dataset is in English.</p> <table><thead><tr><th>Model</th> <th style="text-align:center;">F1 Score</th> <th>Paper / Source</th> <th>Code</th></tr></thead> <tbody><tr><td>5-layer convolutional network (Krizhevsky et al., 2012), Word2Vec</td> <td style="text-align:center;">0.79</td> <td><a href="http://www.aclweb.org/anthology/N16-1020" target="_blank" rel="noopener noreferrer">Shutova et. al, 2016<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td>Unavailable</td></tr></tbody></table> <h2 id="multimodal-sentiment-analysis"><a href="#multimodal-sentiment-analysis" class="header-anchor">#</a> Multimodal Sentiment Analysis</h2> <h3 id="mosi"><a href="#mosi" class="header-anchor">#</a> MOSI</h3> <p>The MOSI dataset (<a href="https://arxiv.org/pdf/1606.06259.pdf" target="_blank" rel="noopener noreferrer">Zadeh et al., 2016<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>) is a dataset rich in sentimental expressions where 93 people review topics in English. The videos are segmented with each segments sentiment label scored between +3 (strong positive) to -3 (strong negative)  by  5  annotators.</p> <table><thead><tr><th>Model</th> <th style="text-align:center;">Accuracy</th> <th>Paper / Source</th></tr></thead> <tbody><tr><td>bc-LSTM (Poria et al., 2017)</td> <td style="text-align:center;">80.3%</td> <td><a href="http://sentic.net/context-dependent-sentiment-analysis-in-user-generated-videos.pdf" target="_blank" rel="noopener noreferrer">Context-Dependent Sentiment Analysis in User-Generated Videos<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr> <tr><td>MARN (Zadeh et al., 2018)</td> <td style="text-align:center;">77.1%</td> <td><a href="https://arxiv.org/pdf/1802.00923.pdf" target="_blank" rel="noopener noreferrer">Multi-attention Recurrent Network for Human Communication Comprehension<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr></tbody></table> <p><a href="/github-nlp-progress/" class="router-link-active">Go back to the README</a></p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div> <footer class="bk-dark" data-v-2be041bb><div class="footer-content" data-v-2be041bb><p data-v-2be041bb>Published with GitHub Pages</p></div></footer></div><div class="global-ui"></div></div>
    <script src="/github-nlp-progress/assets/js/app.e13193a8.js" defer></script><script src="/github-nlp-progress/assets/js/2.3456af6e.js" defer></script><script src="/github-nlp-progress/assets/js/3.937c9f03.js" defer></script><script src="/github-nlp-progress/assets/js/27.cd03ef98.js" defer></script>
  </body>
</html>
